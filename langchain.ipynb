{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GETTING A SIMPLE OUTPUT\n",
    "# üî• What is `temperature` in Language Models?\n",
    "\n",
    "`temperature` is a parameter that controls **how random or creative** the model's output is.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Behavior\n",
    "\n",
    "| Temperature | Description                      | Example Behavior                                             |\n",
    "|-------------|----------------------------------|--------------------------------------------------------------|\n",
    "| `0`         | Deterministic, least random      | Always gives the **same** answer to the same question        |\n",
    "| `~0.3`      | Low randomness, more factual     | Still focused, with minor variation                          |\n",
    "| `0.7`       | Balanced creativity               | Good for storytelling, jokes, general use                    |\n",
    "| `1.0`       | High randomness                   | Very creative, more unpredictable                            |\n",
    "| `>1.0`      | Chaotic or less relevant         | Often diverges from factual answers                          |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example\n",
    "\n",
    "**Prompt:**\n"
   ],
   "id": "7be3833b7cb92d45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:18:44.896773Z",
     "start_time": "2025-06-17T18:18:40.390612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "print(llm.invoke(\"Give solfege notes of Perfect by Ed sheeran\"))\n"
   ],
   "id": "bbab745d2d5f7da7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The song \"Perfect\" by Ed Sheeran is a beautiful ballad. Here are the solfege notes for the main melody:\n",
      "\n",
      "C - E - G - C\n",
      "E - G - B - E\n",
      "G - A - G - F\n",
      "F - E - D - C\n",
      "\n",
      "Please note that these are just the main melody notes, and there may be additional notes or variations in different sections of the song.\n",
      "\n",
      "In solfege, C represents \"Do\", so the correct solfege notes for the above melody would be:\n",
      "\n",
      "Do - Mi - So - Do\n",
      "Mi - So - La - Do\n",
      "So - Fa - Sol - Do\n",
      "Fa - Mi - Re - Do\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PROJECT TEMPLATE AND CHAINS IN LLMS",
   "id": "768ef288aad26518"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:51:47.862722Z",
     "start_time": "2025-06-18T06:51:28.841681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain  # ‚úÖ Correct class\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load local Ollama model\n",
    "llm = Ollama(model=\"llama3.2\", temperature=0.3)\n",
    "\n",
    "# Define prompt template\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {place}?\")\n",
    "\n",
    "# Create the chain using model and prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain with input\n",
    "output = chain.run(\"Burundi\")\n",
    "\n",
    "# Print result\n",
    "print(\"üß†\", output)\n"
   ],
   "id": "658e878174c8efbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† The capital of Burundi is Gitega. However, Bujumbura was previously the capital from 1962 to 2018. In 2018, the government moved the capital to Gitega due to concerns about Bujumbura's instability and poor infrastructure.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:18:47.681215Z",
     "start_time": "2025-06-17T18:18:45.508181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you want to pass multiple inputs as a list\n",
    "cities=[\"India\",\"USA\",\"Turkey\",\"Monaco\"]\n",
    "for city in cities:\n",
    "    output = chain.run(city)\n",
    "    print(output)"
   ],
   "id": "a1c199b3de74bb7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi.\n",
      "The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).\n",
      "The capital of Turkey is Ankara.\n",
      "The capital of Monaco is Monaco City (also known as Monte Carlo).\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîó Simple `SequentialChain` with LangChain and Ollama\n",
    "\n",
    "## üìò What is `SequentialChain`?\n",
    "\n",
    "`SequentialChain` is a LangChain component that allows you to:\n",
    "- **Run multiple chains in sequence**\n",
    "- Automatically **pass the output of one chain** as the input to the next\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Use Case Example\n",
    "\n",
    "We'll create a 2-step chain:\n",
    "1. **Summarize a topic**\n",
    "2. **Generate a tweet** from that summary\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Code Example\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# Step 0: Load your locally running Ollama model (e.g., llama3)\n",
    "llm = Ollama(model=\"llama3\", temperature=0.7)\n",
    "\n",
    "# Step 1: Chain to generate a summary from a topic\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize the topic: {topic}\")\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"summary\")\n",
    "\n",
    "# Step 2: Chain to write a tweet from the summary\n",
    "tweet_prompt = PromptTemplate.from_template(\"Write a tweet based on this summary: {summary}\")\n",
    "tweet_chain = LLMChain(llm=llm, prompt=tweet_prompt, output_key=\"tweet\")\n",
    "\n",
    "# Combine both into a sequential chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[summary_chain, tweet_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"summary\", \"tweet\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain with a topic\n",
    "result = overall_chain.run({\"topic\": \"Benefits of drinking water\"})\n",
    "\n",
    "# Print the outputs\n",
    "print(\"üí° Summary:\", result[\"summary\"])\n",
    "print(\"üê¶ Tweet:\", result[\"tweet\"])\n"
   ],
   "id": "487b7422dfcd0da2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:18:55.375271Z",
     "start_time": "2025-06-17T18:18:47.703692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# Load local Ollama model\n",
    "llm = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "\n",
    "# Step 1: Get summary of lyrics from song name\n",
    "lyrics_prompt = PromptTemplate.from_template(\"Summarize the lyrics of the song: {song}\")\n",
    "lyrics_chain = LLMChain(llm=llm, prompt=lyrics_prompt, output_key=\"lyrics\")\n",
    "\n",
    "# Step 2: Get genre from lyrics\n",
    "genre_prompt = PromptTemplate.from_template(\"Identify the genre of the song based on these lyrics:\\n\\n{lyrics}\")\n",
    "genre_chain = LLMChain(llm=llm, prompt=genre_prompt, output_key=\"genre\")\n",
    "\n",
    "# Combine into a sequential chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[lyrics_chain, genre_chain],\n",
    "    input_variables=[\"song\"],\n",
    "    output_variables=[\"lyrics\", \"genre\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = overall_chain.invoke({\"song\": \"Lag ja gale by Asha Bhosle\"})\n",
    "\n",
    "print(\"üí° Lyrics Summary:\\n\", result[\"lyrics\"])\n",
    "print(\"üéµ Genre:\\n\", result[\"genre\"])\n"
   ],
   "id": "215378d1adb36e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "üí° Lyrics Summary:\n",
      " I couldn't find any information on a song called \"Lag Ja Gale\" by Asha Bhosle. However, I did find that it is a popular Bollywood song from the 1960s, and it was sung by Lata Mangeshkar, not Asha Bhosle.\n",
      "\n",
      "The lyrics of \"Lag Ja Gale\" are about a person who has to leave their beloved behind due to circumstances beyond their control. The song's theme is one of longing and separation, with the singer expressing their emotions of sadness and heartache at having to part ways with their loved one.\n",
      "\n",
      "If you're interested, I can try to provide more information on the song or its original context in a Bollywood movie.\n",
      "üéµ Genre:\n",
      " Based on the lyrics provided, I would categorize the genre of \"Lag Ja Gale\" as:\n",
      "\n",
      "1. Bollywood Film Song\n",
      "2. Ballad (specifically, a classic Indian film ballad)\n",
      "3. Hindi Music\n",
      "4. Romantic Ballad/Movie Song\n",
      "\n",
      "The song's theme of longing and separation, with emotions of sadness and heartache, is characteristic of many traditional Bollywood film songs from the 1960s.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:19:24.102235Z",
     "start_time": "2025-06-17T18:18:55.405195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import lyricsgenius\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Step 0: Fetch lyrics using Genius API\n",
    "genius = lyricsgenius.Genius(\"6zjxdsJMvjbAOVYWfIJ0o2cb5y_Dx0mWXwg_7aJDfKpBcr4VrNBAfN_TPZE4Ob2y\", remove_section_headers=True)\n",
    "song_obj = genius.search_song(\"Bulleya\", \"Arijit Singh\")\n",
    "\n",
    "# Get lyrics or fallback\n",
    "lyrics = song_obj.lyrics if song_obj and song_obj.lyrics else \"Lyrics not found.\"\n",
    "\n",
    "# Step 1: Load local Ollama model\n",
    "llm = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "\n",
    "# Step 2: Summarize lyrics\n",
    "lyrics_prompt = PromptTemplate.from_template(\"Summarize the following lyrics:\\n\\n{lyrics}\")\n",
    "lyrics_chain = LLMChain(llm=llm, prompt=lyrics_prompt, output_key=\"summary\")\n",
    "\n",
    "# Step 3: Identify genre\n",
    "genre_prompt = PromptTemplate.from_template(\"Identify the genre of the song based on this summary , give a single word output:\\n\\n{summary}\")\n",
    "genre_chain = LLMChain(llm=llm, prompt=genre_prompt, output_key=\"genre\")\n",
    "\n",
    "# Step 4: Combine into a chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[lyrics_chain, genre_chain],\n",
    "    input_variables=[\"lyrics\"],\n",
    "    output_variables=[\"summary\", \"genre\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 5: Run the chain\n",
    "result = overall_chain.invoke({\"lyrics\": lyrics})\n",
    "\n",
    "# Step 6: Print everything\n",
    "print(\"\\nüé§ Original Lyrics:\\n\", lyrics)\n",
    "print(\"\\nüí° Lyrics Summary:\\n\", result[\"summary\"])\n",
    "print(\"\\nüéµ Genre:\\n\", result[\"genre\"])\n"
   ],
   "id": "3ddcb46b422cc0c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Bulleya\" by Arijit Singh...\n",
      "Done.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "üé§ Original Lyrics:\n",
      " 2 ContributorsTranslationsRomanizationBulleya (Reprise) Lyrics\n",
      "‡§Æ‡•á‡§∞‡•Ä ‡§∞‡•Ç‡§π ‡§ï‡§æ ‡§™‡§∞‡§ø‡§Ç‡§¶‡§æ ‡§´‡•ú‡§´‡•ú‡§æ‡§è\n",
      "‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§ï‡§æ ‡§ú‡•õ‡•Ä‡§∞‡§æ ‡§Æ‡§ø‡§≤ ‡§®‡§æ ‡§™‡§æ‡§è\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡•ã ‡§§‡§ú‡§≤‡•ç‡§≤‡•Ä ‡§§‡•ã ‡§¶‡§ø‡§ñ‡§æ ‡§¶‡•á\n",
      "‡§ù‡•Ç‡§†‡•Ä ‡§∏‡§π‡•Ä, ‡§Æ‡§ó‡§∞ ‡§§‡§∏‡§≤‡•ç‡§≤‡•Ä ‡§§‡•ã ‡§¶‡§ø‡§≤‡§æ ‡§¶‡•á\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "‡®Æ‡©à‡®Ç ‡®§‡®æ‡®Ç ‡®ó‡©Å‡®≤ ‡®∏‡©á ‡®≤‡®ø‡®™‡®ü‡©Ä ‡®§‡®ø‡®§‡®≤‡©Ä ‡®ï‡©Ä ‡®§‡®∞‡©ç‡®π‡®æ‡®Ç ‡®Æ‡©Å‡®π‡®æ‡®ú‡®ø‡®∞ ‡®π‡©Ç‡©∞\n",
      "‡§è‡§ï ‡§™‡§≤ ‡§ï‡•ã ‡§†‡§π‡§∞‡•Ç‡§Å, ‡§™‡§≤ ‡§Æ‡•á‡§Ç ‡§â‡•ú ‡§ú‡§æ‡§ä‡§Å\n",
      "‡®µ‡©á ‡®Æ‡©à‡®Ç ‡®§‡®æ‡®Ç ‡®π‡©Ç‡©∞ ‡®™‡®ó‡®°‡©∞‡®°‡©Ä, ‡®≤‡©±‡®≠‡®¶‡©Ä ‡®ê ‡®ú‡©ã ‡®∞‡®æ‡®π ‡®ú‡©∞‡®®‡®§ ‡®ï‡©Ä\n",
      "‡§§‡•Ç ‡§Æ‡•Å‡•ú‡•á ‡§ú‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§∏‡§æ‡§• ‡§Æ‡•Å‡•ú ‡§ú‡§æ‡§ä‡§Å\n",
      "\n",
      "‡§§‡•á‡§∞‡•á ‡§ï‡§æ‡§∞‡§µ‡§æ‡§Å ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "‡§ï‡§Æ‡§ø‡§Ø‡§æ‡§Å ‡§§‡§∞‡§æ‡§∂ ‡§ï‡•á ‡§Æ‡•à‡§Ç ‡•ò‡§æ‡§¨‡§ø‡§≤ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ ‡®µ‡©á, ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ ‡®µ‡©á\n",
      "\n",
      "‡§ú‡§ø‡§∏ ‡§¶‡§ø‡§® ‡§∏‡•á ‡§Ü‡§∂‡§®‡§æ ‡§∏‡•á ‡§¶‡•ã ‡§Ö‡§ú‡§®‡§¨‡•Ä ‡§π‡•Å‡§è ‡§π‡•à‡§Ç\n",
      "‡§§‡§®‡§π‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§Æ‡§π‡•á ‡§∏‡§¨ ‡§Æ‡•Å‡§≤‡§§‡§µ‡•Ä ‡§π‡•Å‡§è ‡§π‡•à‡§Ç\n",
      "‡§ï‡•ç‡§Ø‡•Ç‡§Å ‡§Ü‡§ú ‡§Æ‡•à‡§Ç ‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§ ‡§´‡§ø‡§∞ ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å?\n",
      "‡§Ø‡•á ‡§¶‡§ø‡§≤ ‡§§‡•ã ‡§¢‡•Ç‡§Å‡§¢‡§§‡§æ ‡§π‡•à ‡§á‡§®‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§π‡§æ‡§®‡•á\n",
      "‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§ú‡§ø‡§∏‡•ç‡§Æ ‡§ï‡•ã‡§à ‡§™‡§æ‡§¨‡§Ç‡§¶‡§ø‡§Ø‡§æ‡§Å ‡§®‡§æ ‡§Æ‡§æ‡§®‡•á\n",
      "‡§Æ‡§ø‡§≤ ‡§ï‡•á ‡§§‡•Å‡§ù‡•á, ‡§¨‡•ö‡§æ‡§µ‡§§ ‡§ñ‡•Å‡§¶ ‡§∏‡•á ‡§π‡•Ä ‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "\n",
      "‡§Æ‡•Å‡§ù‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡§® ‡§π‡•à ‡§¨‡§æ‡•ò‡•Ä, ‡§Ü‡•õ‡§Æ‡§æ ‡§≤‡•á\n",
      "‡§≤‡•á ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•Ç‡§Å ‡§ñ‡•Å‡§¶ ‡§ï‡•ã ‡§Æ‡•à‡§Ç ‡§§‡•á‡§∞‡•á ‡§π‡§µ‡§æ‡§≤‡•á\n",
      "‡®µ‡©á ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ, ‡®µ‡©á ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "üí° Lyrics Summary:\n",
      " These are the lyrics to a song called \"Bulleya (Reprise)\" in Punjabi. The song appears to be about unrequited love and longing for someone who is out of reach.\n",
      "\n",
      "The song begins with the lines:\n",
      "\n",
      "\"‡§Æ‡•á‡§∞‡•Ä ‡§∞‡•Ç‡§π ‡§ï‡§æ ‡§™‡§∞‡§ø‡§Ç‡§¶‡§æ ‡§´‡•ú‡§´‡•ú‡§æ‡§è / ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§ï‡§æ ‡§ú‡•õ‡•Ä‡§∞‡§æ ‡§Æ‡§ø‡§≤ ‡§®‡§æ ‡§™‡§æ‡§è\"\n",
      "\n",
      "Which translates to \"My soul's wings flutter, but no peace is found.\"\n",
      "\n",
      "The singer is asking themselves what they should do, as they are yearning for someone who may not be reciprocating their feelings.\n",
      "\n",
      "Throughout the song, there are repeated lines and phrases that emphasize the longing and desperation. The chorus, \"‡§µ‡•á ‡§ï‡•Ä ‡§ï‡§∞‡§æ‡•Ö‡§Ç? ‡§µ‡•á ‡§ï‡•Ä ‡§ï‡§∞‡§æ‡•Ö‡§Ç?\" (\"What shall I do? What shall I do?\"), is a refrain that is sung multiple times throughout the song.\n",
      "\n",
      "The lyrics also contain references to the concept of \"Ranjha\" (a Punjabi term for a lover or sweetheart), with lines like:\n",
      "\n",
      "\"‡§∞‡§æ‡§Ç‡§ù‡§® ‡§¶‡•á ‡§Ø‡§æ‡§∞ ‡§¨‡•Å‡§≤‡•ç‡§π‡§ø‡§Ü, ‡§∏‡•Å‡§® ‡§≤‡•á ‡§™‡•Å‡§ï‡§æ‡§∞ ‡§¨‡•Å‡§≤‡•ç‡§π‡§ø‡§Ü\"\n",
      "\n",
      "Which translates to \"Ranjha's friend is calling out, listen to the call.\"\n",
      "\n",
      "The singer is addressing their loved one, asking them to respond and acknowledging that they are the only one who can make them feel alive.\n",
      "\n",
      "The song also contains themes of heartbreak, loss, and the pain of loving someone who is unattainable. The repetition of the phrase \"‡§Æ‡•à‡§Ç ‡§§‡•ã ‡§ó‡•Å‡§≤ ‡§∏‡•á ‡§≤‡§ø‡§™‡§ü‡•Ä ‡§§‡§ø‡§§‡§≤‡•Ä ‡§ï‡§ø ‡§§‡§∞‡•ç‡§ß‡§æ‡§Å ‡§Æ‡•Å‡§π‡§æ‡§ú‡§ø‡§∞ ‡§π‡•Ç‡§Å\" (\"I am just a flower that's fallen from its stem\") emphasizes the singer's sense of vulnerability and longing.\n",
      "\n",
      "Overall, the song is a poignant expression of unrequited love and the pain of loving someone who is out of reach.\n",
      "\n",
      "üéµ Genre:\n",
      " Ballad\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:52:23.797483Z",
     "start_time": "2025-06-18T06:52:18.217918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import lyricsgenius\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Step 0: Fetch lyrics using Genius API\n",
    "genius = lyricsgenius.Genius(\"6zjxdsJMvjbAOVYWfIJ0o2cb5y_Dx0mWXwg_7aJDfKpBcr4VrNBAfN_TPZE4Ob2y\", remove_section_headers=True)\n",
    "song_obj = genius.search_song(\"Bulleya\", \"Arijit Singh\")\n",
    "\n",
    "# Get lyrics or fallback\n",
    "lyrics = song_obj.lyrics if song_obj and song_obj.lyrics else \"Lyrics not found.\"\n",
    "\n",
    "# Step 1: Use Groq's Mixtral model via OpenAI-compatible API\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    openai_api_key=\"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\",\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Step 2: Summarize lyrics\n",
    "lyrics_prompt = PromptTemplate.from_template(\"Summarize the following lyrics:\\n\\n{lyrics}\")\n",
    "lyrics_chain = LLMChain(llm=llm, prompt=lyrics_prompt, output_key=\"summary\")\n",
    "\n",
    "# Step 3: Identify genre\n",
    "genre_prompt = PromptTemplate.from_template(\"Identify the genre of the song based on this summary, give a single word output:\\n\\n{summary}\")\n",
    "genre_chain = LLMChain(llm=llm, prompt=genre_prompt, output_key=\"genre\")\n",
    "\n",
    "# Step 4: Combine into a chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[lyrics_chain, genre_chain],\n",
    "    input_variables=[\"lyrics\"],\n",
    "    output_variables=[\"summary\", \"genre\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 5: Run the chain\n",
    "result = overall_chain.invoke({\"lyrics\": lyrics})\n",
    "\n",
    "# Step 6: Print everything\n",
    "print(\"\\nüé§ Original Lyrics:\\n\", lyrics)\n",
    "print(\"\\nüí° Lyrics Summary:\\n\", result[\"summary\"])\n",
    "print(\"\\nüéµ Genre:\\n\", result[\"genre\"])\n"
   ],
   "id": "8678131c52d8f5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Bulleya\" by Arijit Singh...\n",
      "Done.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "üé§ Original Lyrics:\n",
      " 2 ContributorsTranslationsRomanizationBulleya (Reprise) Lyrics\n",
      "‡§Æ‡•á‡§∞‡•Ä ‡§∞‡•Ç‡§π ‡§ï‡§æ ‡§™‡§∞‡§ø‡§Ç‡§¶‡§æ ‡§´‡•ú‡§´‡•ú‡§æ‡§è\n",
      "‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§ï‡§æ ‡§ú‡•õ‡•Ä‡§∞‡§æ ‡§Æ‡§ø‡§≤ ‡§®‡§æ ‡§™‡§æ‡§è\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡•ã ‡§§‡§ú‡§≤‡•ç‡§≤‡•Ä ‡§§‡•ã ‡§¶‡§ø‡§ñ‡§æ ‡§¶‡•á\n",
      "‡§ù‡•Ç‡§†‡•Ä ‡§∏‡§π‡•Ä, ‡§Æ‡§ó‡§∞ ‡§§‡§∏‡§≤‡•ç‡§≤‡•Ä ‡§§‡•ã ‡§¶‡§ø‡§≤‡§æ ‡§¶‡•á\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "‡®Æ‡©à‡®Ç ‡®§‡®æ‡®Ç ‡®ó‡©Å‡®≤ ‡®∏‡©á ‡®≤‡®ø‡®™‡®ü‡©Ä ‡®§‡®ø‡®§‡®≤‡©Ä ‡®ï‡©Ä ‡®§‡®∞‡©ç‡®π‡®æ‡®Ç ‡®Æ‡©Å‡®π‡®æ‡®ú‡®ø‡®∞ ‡®π‡©Ç‡©∞\n",
      "‡§è‡§ï ‡§™‡§≤ ‡§ï‡•ã ‡§†‡§π‡§∞‡•Ç‡§Å, ‡§™‡§≤ ‡§Æ‡•á‡§Ç ‡§â‡•ú ‡§ú‡§æ‡§ä‡§Å\n",
      "‡®µ‡©á ‡®Æ‡©à‡®Ç ‡®§‡®æ‡®Ç ‡®π‡©Ç‡©∞ ‡®™‡®ó‡®°‡©∞‡®°‡©Ä, ‡®≤‡©±‡®≠‡®¶‡©Ä ‡®ê ‡®ú‡©ã ‡®∞‡®æ‡®π ‡®ú‡©∞‡®®‡®§ ‡®ï‡©Ä\n",
      "‡§§‡•Ç ‡§Æ‡•Å‡•ú‡•á ‡§ú‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§∏‡§æ‡§• ‡§Æ‡•Å‡•ú ‡§ú‡§æ‡§ä‡§Å\n",
      "\n",
      "‡§§‡•á‡§∞‡•á ‡§ï‡§æ‡§∞‡§µ‡§æ‡§Å ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "‡§ï‡§Æ‡§ø‡§Ø‡§æ‡§Å ‡§§‡§∞‡§æ‡§∂ ‡§ï‡•á ‡§Æ‡•à‡§Ç ‡•ò‡§æ‡§¨‡§ø‡§≤ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç? ‡®µ‡©á ‡®ï‡©Ä ‡®ï‡®∞‡®æ‡®Ç?\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ ‡®µ‡©á, ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ ‡®µ‡©á\n",
      "\n",
      "‡§ú‡§ø‡§∏ ‡§¶‡§ø‡§® ‡§∏‡•á ‡§Ü‡§∂‡§®‡§æ ‡§∏‡•á ‡§¶‡•ã ‡§Ö‡§ú‡§®‡§¨‡•Ä ‡§π‡•Å‡§è ‡§π‡•à‡§Ç\n",
      "‡§§‡§®‡§π‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§Æ‡§π‡•á ‡§∏‡§¨ ‡§Æ‡•Å‡§≤‡§§‡§µ‡•Ä ‡§π‡•Å‡§è ‡§π‡•à‡§Ç\n",
      "‡§ï‡•ç‡§Ø‡•Ç‡§Å ‡§Ü‡§ú ‡§Æ‡•à‡§Ç ‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§ ‡§´‡§ø‡§∞ ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å?\n",
      "‡§Ø‡•á ‡§¶‡§ø‡§≤ ‡§§‡•ã ‡§¢‡•Ç‡§Å‡§¢‡§§‡§æ ‡§π‡•à ‡§á‡§®‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§π‡§æ‡§®‡•á\n",
      "‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§ú‡§ø‡§∏‡•ç‡§Æ ‡§ï‡•ã‡§à ‡§™‡§æ‡§¨‡§Ç‡§¶‡§ø‡§Ø‡§æ‡§Å ‡§®‡§æ ‡§Æ‡§æ‡§®‡•á\n",
      "‡§Æ‡§ø‡§≤ ‡§ï‡•á ‡§§‡•Å‡§ù‡•á, ‡§¨‡•ö‡§æ‡§µ‡§§ ‡§ñ‡•Å‡§¶ ‡§∏‡•á ‡§π‡•Ä ‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Å\n",
      "\n",
      "‡§Æ‡•Å‡§ù‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡§® ‡§π‡•à ‡§¨‡§æ‡•ò‡•Ä, ‡§Ü‡•õ‡§Æ‡§æ ‡§≤‡•á\n",
      "‡§≤‡•á ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•Ç‡§Å ‡§ñ‡•Å‡§¶ ‡§ï‡•ã ‡§Æ‡•à‡§Ç ‡§§‡•á‡§∞‡•á ‡§π‡§µ‡§æ‡§≤‡•á\n",
      "‡®µ‡©á ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ, ‡®µ‡©á ‡®∞‡®æ‡®Ç‡®ù‡®£‡®æ\n",
      "\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®∞‡®æ‡®Ç‡®ù‡®£ ‡®¶‡©á ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®∏‡©Å‡®® ‡®≤‡©á ‡®™‡©Å‡®ï‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®§‡©Ç‡©∞ ‡®π‡©Ä ‡®§‡©ã ‡®Ø‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ (‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ)\n",
      "‡®§‡©á‡®∞‡®æ ‡®Æ‡©Å‡®ï‡®æ‡®Æ ‡®ï‡®Æ‡®≤‡©á ‡®∏‡®∞‡®π‡©±‡®¶ ‡®ï‡©á ‡®™‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü\n",
      "‡®™‡®∞‡®µ‡®∞‡®¶‡®ø‡®ó‡®æ‡®∞ ‡®¨‡©Å‡®≤‡©ç‡®π‡®ø‡®Ü, ‡®π‡®æ‡©û‡®ø‡©õ ‡®§‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ, ‡®Æ‡©Å‡®∞‡®∂‡®ø‡®¶ ‡®Æ‡©á‡®∞‡®æ\n",
      "\n",
      "üí° Lyrics Summary:\n",
      " The song \"Bulleya\" is a spiritual and introspective ballad that explores themes of love, longing, and self-discovery. The lyrics are written in a mix of Hindi, Urdu, and Punjabi, which adds to the song's mystique.\n",
      "\n",
      "The song's narrator is searching for inner peace and solace, seeking comfort in the presence of a higher power or a spiritual guide (referred to as \"Murshid Mere\" or \"My Spiritual Guide\"). They express a deep sense of longing and yearning, comparing their restless soul to a bird that cannot find solace.\n",
      "\n",
      "The lyrics also contain elements of Sufi poetry, with the narrator seeking union with the divine and expressing a desire to transcend worldly attachments. The repetition of the phrase \"Ranjhana\" (a spiritual guide or beloved) emphasizes the narrator's quest for spiritual enlightenment.\n",
      "\n",
      "Throughout the song, the narrator grapples with the contradictions of human nature, acknowledging the coexistence of conflicting desires and impulses. They express a desire to break free from worldly constraints and find freedom in the pursuit of love and spiritual growth.\n",
      "\n",
      "Ultimately, the lyrics convey a profound introspection and a deep sense of spiritual longing, as the narrator seeks to find their way through the complexities of the human experience.\n",
      "\n",
      "üéµ Genre:\n",
      " Sufi\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîÅ SimpleSequentialChain vs SequentialChain in LangChain\n",
    "\n",
    "LangChain provides two main ways to chain multiple steps together:\n",
    "- `SimpleSequentialChain`\n",
    "- `SequentialChain`\n",
    "\n",
    "Both pass the output of one step to the next, but they differ in **flexibility**, **features**, and **complexity**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Feature Comparison\n",
    "\n",
    "| Feature                            | `SimpleSequentialChain`                                                                 | `SequentialChain`                                                                                      |\n",
    "|------------------------------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Purpose**                        | Simplifies chaining of multiple components sequentially                                 | Enables more complex, structured chaining with named inputs/outputs                                      |\n",
    "| **Input/Output**                   | Accepts a single input and returns a single output                                       | Supports multiple named inputs and outputs                                                              |\n",
    "| **Chaining Style**                 | Only output of one chain is passed as input to the next                                 | Each step can access multiple previous inputs and outputs                                               |\n",
    "| **Custom Logic Between Steps**     | ‚ùå Not possible                                                                           | ‚úÖ Fully customizable logic between steps                                                               |\n",
    "| **Use Case**                       | Linear, single-input-to-output chains (e.g., summarizing ‚Üí translating ‚Üí rephrasing)    | Complex workflows where intermediate data is needed between steps                                       |\n",
    "| **Intermediate Output Access**     | ‚ùå No access to intermediate outputs                                                      | ‚úÖ Can store and access intermediate step outputs                                                       |\n",
    "| **Configuration Complexity**       | ‚≠ê Very simple                                                                            | ‚öôÔ∏è More complex (requires explicit input/output variable mapping)                                       |\n",
    "| **Example Use Case**               | - Convert text to summary, then rephrase summary                                         | - Use output from Step 1 and original input in Step 2                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Usage\n",
    "\n",
    "### üîπ SimpleSequentialChain Example\n",
    "\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "# Step 1: Summarize\n",
    "prompt1 = PromptTemplate.from_template(\"Summarize this: {input}\")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "# Step 2: Translate\n",
    "prompt2 = PromptTemplate.from_template(\"Translate this into French: {input}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "simple_chain = SimpleSequentialChain(chains=[chain1, chain2])\n",
    "simple_chain.run(\"Virat Kohli scored a century in the World Cup final.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ SequentialChain Example\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"input\"],         # names used in first chain\n",
    "    output_variables=[\"text\"],         # final output variable name\n",
    "    verbose=True,\n",
    ")\n",
    "sequential_chain.run({\"input\": \"Virat Kohli scored a century in the World Cup final.\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use Which?\n",
    "\n",
    "| Use Case                                      | Use `SimpleSequentialChain` if...                     | Use `SequentialChain` if...                                        |\n",
    "|-----------------------------------------------|--------------------------------------------------------|---------------------------------------------------------------------|\n",
    "| Quick chaining of simple steps                | ...you have single string inputs/outputs only         | ...you need named variables or multiple step logic                 |\n",
    "| Access to intermediate values needed          | ‚ùå Not supported                                       | ‚úÖ Supported                                                        |\n",
    "| Debugging and verbose logging required        | ‚ùå Not available                                       | ‚úÖ Use `verbose=True`                                               |\n",
    "| You need reusability and modularity           | ‚ùå Not ideal                                           | ‚úÖ Best suited                                                      |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "2008dc3ac0c9dfbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ü§ñ LangChain Agents\n",
    "\n",
    "LangChain **Agents** are powerful components that allow language models to **dynamically decide** what actions to take using external tools to solve complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Are Agents in LangChain?\n",
    "\n",
    "**Agents** use an LLM to:\n",
    "- Reason through a task,\n",
    "- Decide what steps to take next,\n",
    "- Call tools or functions dynamically,\n",
    "- Continue until a final answer is found.\n",
    "\n",
    "> Unlike `Chains` (which have a fixed path), **Agents plan their own steps** based on the task.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Agents vs Chains\n",
    "\n",
    "| Feature               | Chains                                    | Agents                                                     |\n",
    "|-----------------------|-------------------------------------------|-------------------------------------------------------------|\n",
    "| **Flow Type**         | Static, predefined sequence               | Dynamic, LLM decides next step                              |\n",
    "| **Suitable For**      | Known, repeatable steps                   | Unknown, complex, multi-step reasoning                      |\n",
    "| **Tool Usage**        | Fixed sequence                            | Chosen on-the-fly by the agent                             |\n",
    "| **Flexibility**       | ‚ùå Limited                                 | ‚úÖ Very flexible                                            |\n",
    "| **Example Use Case**  | Summarize ‚Üí Translate ‚Üí Rephrase          | \"Find the weather in Tokyo and convert it to Fahrenheit\"   |\n",
    "\n",
    "---\n",
    "\n",
    "## üî® Components of an Agent\n",
    "\n",
    "1. **LLM** ‚Äì Acts as the reasoning engine (e.g., GPT-4, Claude).\n",
    "2. **Tools** ‚Äì External functions the agent can use.\n",
    "3. **Agent Type** ‚Äì Defines the control logic (e.g., `zero-shot-react`, `chat-conversational-react`).\n",
    "4. **AgentExecutor** ‚Äì Orchestrates everything together.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Common Tools Used by Agents\n",
    "\n",
    "| Tool               | Description                                           |\n",
    "|--------------------|-------------------------------------------------------|\n",
    "| `SerpAPI`          | For real-time web search                              |\n",
    "| `LLMMath`          | Solves math problems using the LLM                    |\n",
    "| `Python REPL`      | Runs Python code                                      |\n",
    "| `Requests`         | Sends HTTP requests to web APIs                       |\n",
    "| `SQL Database`     | Query structured databases                            |\n",
    "| `VectorStore`      | Searches in vector-based document storage             |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Example Agent Code\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Define the LLM from Groq (LLaMA3 model)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    openai_api_key=\"Replace it with your key\",  # Replace with env var or secure method\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define tools\n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "llm_math = LLMMathChain(llm=llm)\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"wikipedia\", func=wiki.run, description=\"Search Wikipedia for facts and people\"),\n",
    "    Tool(name=\"Calculator\", func=llm_math.run, description=\"Do math calculations\")\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"Area of New York in sq km and what is 2 +3?\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use Agents?\n",
    "\n",
    "Use Agents if:\n",
    "- The task has **unknown steps** or requires **external tools**\n",
    "- You need **dynamic decision-making**\n",
    "- You want the model to **plan its own approach**\n",
    "\n",
    "Avoid Agents if:\n",
    "- The steps are **fixed** and predictable\n",
    "- Performance is critical (agents can be slower)\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "542cebdc3cdbd709"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:38:51.194528Z",
     "start_time": "2025-06-18T05:38:32.596781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Define the LLM from Groq (LLaMA3 model)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    openai_api_key=\"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\",  # Replace with env var or secure method\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define tools\n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "llm_math = LLMMathChain(llm=llm)\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"wikipedia\", func=wiki.run, description=\"Search Wikipedia for facts and people\"),\n",
    "    # Tool(name=\"Calculator\", func=llm_math.run, description=\"Do math calculations\")\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What is Thapar Institute of Engineering and Technology and whats its fees?\")\n"
   ],
   "id": "565482c7b8ea379f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thapar Institute of Engineering and Technology is a private deemed-to-be-university in Patiala, India, but I couldn't find the fees of Thapar Institute of Engineering and Technology on Wikipedia.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION`\n",
    "  üëâ Uses the **ReAct (Reason + Act)** agent that decides actions using descriptions of tools **without examples** (zero-shot).\n",
    "\n",
    "- `verbose=False`\n",
    "  üëâ Disables step-by-step logging of the agent's thoughts, actions, and observations.\n",
    "- `SERPER_API`\n",
    "  üëâ For realtime google search we can use the SERPApi Tool.\n"
   ],
   "id": "3271b2c74135a689"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# üß† Memory in LangChain\n",
    "\n",
    "In LangChain, **Memory** refers to mechanisms that let language models **remember information across interactions**. This is essential for building stateful applications like chatbots, personal assistants, or any multi-turn conversations.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why Use Memory?\n",
    "\n",
    "- Maintain **conversation history**\n",
    "- Enable **contextual responses**\n",
    "- Store and retrieve **intermediate outputs or user data**\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Types of Memory in LangChain\n",
    "\n",
    "### 1. **ConversationBufferMemory**\n",
    "\n",
    "- Stores entire conversation history in memory.\n",
    "- Useful for chatbots and assistants.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "```\n",
    "\n",
    "### 2. **ConversationBufferWindowMemory**\n",
    "\n",
    "- Keeps only the last `k` interactions.\n",
    "- More efficient than full buffer when history is long.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "```\n",
    "\n",
    "### 3. **ConversationTokenBufferMemory**\n",
    "\n",
    "- Similar to `BufferWindow`, but limits memory by **token count** rather than turns.\n",
    "- Helps avoid hitting token limits.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=1000)\n",
    "```\n",
    "\n",
    "### 4. **ConversationSummaryMemory**\n",
    "\n",
    "- Uses the LLM to summarize older messages and retains a summary + latest exchanges.\n",
    "- Ideal for long conversations without losing high-level context.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "```\n",
    "\n",
    "### 5. **CombinedMemory**\n",
    "\n",
    "- Combines multiple memory strategies together.\n",
    "- Example: keep a summary and recent tokens both.\n",
    "\n",
    "```python\n",
    "from langchain.memory import CombinedMemory\n",
    "\n",
    "memory = CombinedMemory(memories=[summary_memory, token_memory])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Using Memory with Chains\n",
    "\n",
    "Memory can be attached to chains using the `memory` parameter.\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Custom Memory\n",
    "\n",
    "You can implement your own memory class by inheriting from `BaseMemory` and implementing:\n",
    "\n",
    "- `load_memory_variables`\n",
    "- `save_context`\n",
    "- `clear`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use Which?\n",
    "\n",
    "| Memory Type                | Best For                             |\n",
    "|---------------------------|--------------------------------------|\n",
    "| BufferMemory              | Small, short chats                   |\n",
    "| BufferWindowMemory        | Recent context only                  |\n",
    "| TokenBufferMemory         | Preventing token overflow            |\n",
    "| SummaryMemory             | Large conversations                  |\n",
    "| CombinedMemory            | Hybrid use cases                     |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Use Case\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "response = chain.run(\"Hello, what's your name?\")\n",
    "```\n"
   ],
   "id": "7450da615b2b7c8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:28:37.311239Z",
     "start_time": "2025-06-18T06:28:36.359652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    openai_api_key=\"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\",  # Replace with env var or secure method\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    temperature=0.7\n",
    ")\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "response = chain.run(\"Hello, what's the date today?\")\n",
    "response2 = chain.run(\"Hello, what's the day today?\")\n",
    "print(chain.memory.buffer)# keeps the memory of the chats done with the agent\n",
    "# print(response)"
   ],
   "id": "aa336bcf03816ddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, what's the date today?\n",
      "AI: Hello there! I'm thrilled to be chatting with you! According to my internal clock, which is synchronized with Coordinated Universal Time (UTC), the current date is Thursday, March 16, 2023. Would you like to know more about this day, like the astronomical twilight times or the phases of the moon?\n",
      "Human: Hello, what's the day today?\n",
      "AI: Easy one! Today is Thursday, March 16, 2023. Did you know that Thursday is named after the Norse god of thunder, Thor? In many European cultures, Thursday is considered a lucky day. Would you like to know more about the cultural significance of Thursday?\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# üìö Vector Databases Explained\n",
    "\n",
    "Vector databases are purpose-built databases designed to **store, index, and search vector embeddings** efficiently. They are commonly used in applications like semantic search, recommendation systems, image/audio retrieval, and large language model pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ What is a Vector?\n",
    "\n",
    "A **vector** is a list of numbers that represents data in a high-dimensional space.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "embedding = [0.123, -0.456, 0.789, ..., 0.234]\n",
    "```\n",
    "\n",
    "These are usually generated by **embedding models** (e.g., OpenAI, SentenceTransformers) to represent the semantic meaning of text, images, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Use Vector Databases?\n",
    "\n",
    "- üîç **Efficient Similarity Search** (k-NN, cosine, Euclidean, dot product)\n",
    "- üöÄ Scalable to millions or billions of vectors\n",
    "- üîó Support metadata filtering (e.g., tags, user info)\n",
    "- üì¶ Persistence & versioning of embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "| Feature             | Description                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| Indexing            | Fast approximate nearest neighbor (ANN) search using HNSW, IVF, etc.       |\n",
    "| Filtering           | Search vectors with conditions on metadata                                 |\n",
    "| Scalability         | Handles millions/billions of vectors                                       |\n",
    "| Integration         | Works well with ML/NLP pipelines                                           |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Popular Vector Databases\n",
    "\n",
    "| Name         | Highlights                                |\n",
    "|--------------|-------------------------------------------|\n",
    "| **FAISS**    | Facebook AI, local & fast, open-source    |\n",
    "| **Pinecone** | Fully managed, real-time filtering        |\n",
    "| **Weaviate** | Open-source, RESTful API, hybrid search   |\n",
    "| **Milvus**   | High performance, distributed             |\n",
    "| **Qdrant**   | Rust-based, filtering + payload support   |\n",
    "| **Chroma**   | Simple local dev tool, LangChain-native   |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Use Case\n",
    "\n",
    "### Storing Embeddings from Documents\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load and embed documents\n",
    "documents = TextLoader(\"docs.txt\").load()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store in FAISS vector store\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"What is LangChain?\"\n",
    "results = db.similarity_search(query)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How Vector Search Works\n",
    "\n",
    "1. Convert input query to an **embedding** vector.\n",
    "2. Compare it with stored vectors using **distance metrics**:\n",
    "   - Cosine Similarity\n",
    "   - Euclidean Distance\n",
    "   - Dot Product\n",
    "3. Return **top-k** most similar vectors (documents/images/etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Use in LangChain\n",
    "\n",
    "LangChain supports:\n",
    "- Storing `Document` objects with embeddings\n",
    "- Retrieving top matches for question answering\n",
    "- Metadata filtering + combining with retriever chains\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "- Vector DBs are essential for semantic search & LLM apps\n",
    "- They allow fast, intelligent retrieval based on meaning, not keywords\n",
    "- Commonly paired with tools like LangChain, OpenAI, HuggingFace\n",
    "\n"
   ],
   "id": "253f332104f8f8ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# üîç Retrieval Using Vector Embeddings & Cosine Similarity\n",
    "\n",
    "## üìå Overview\n",
    "\n",
    "Retrieval using vector embeddings is a technique that allows systems to find semantically relevant data ‚Äî not just based on keywords, but on **meaning**. This is especially useful in applications like semantic search, question answering, and recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Are Vector Embeddings?\n",
    "\n",
    "Vector embeddings are **numerical representations** of data (like text, images, or audio) in a high-dimensional space. They capture **semantic meaning**, so similar items will have vectors that are **close together**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê What is Cosine Similarity?\n",
    "\n",
    "**Cosine similarity** is a metric that calculates how similar two vectors are by measuring the **angle between them**. It ranges from:\n",
    "\n",
    "- **+1** ‚Üí Exactly similar (same direction)\n",
    "- **0** ‚Üí Unrelated (orthogonal)\n",
    "- **-1** ‚Üí Opposite meaning (opposite direction)\n",
    "\n",
    "Cosine similarity is preferred over Euclidean distance when the **magnitude doesn't matter**, only the **direction** of the vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Retrieval Works\n",
    "\n",
    "### 1. **Embed the Query**\n",
    "The user‚Äôs input (e.g., a question or image) is converted into a vector embedding using a model.\n",
    "\n",
    "### 2. **Compare with Stored Vectors**\n",
    "This query vector is compared to all the vectors stored in the vector database, which represent other documents or items.\n",
    "\n",
    "### 3. **Compute Cosine Similarity**\n",
    "For each stored vector, cosine similarity with the query vector is computed to measure semantic closeness.\n",
    "\n",
    "### 4. **Rank the Results**\n",
    "Vectors are ranked from most similar to least similar based on their cosine similarity scores.\n",
    "\n",
    "### 5. **Retrieve Top-k Items**\n",
    "The system returns the **top-k** most relevant items ‚Äî these are the ones whose vectors are **closest in meaning** to the query.\n",
    "\n",
    "### 6. **Hashing of Vectors**\n",
    "We can use a hashing function to segregate all the vectors into **buckets** of all similar vectors  ‚Äî after that the new vector is classified into one of these buckets by passing it through the same vector function and then doing  **linear search** in the bucket in which the input vector is classified into.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why It‚Äôs Powerful\n",
    "\n",
    "- Goes beyond exact keyword matching.\n",
    "- Finds conceptually related content even if the words don‚Äôt match.\n",
    "- Enables LLMs and AI systems to provide smarter and more contextually relevant responses.\n",
    "\n"
   ],
   "id": "c8ed360fa9382c17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
